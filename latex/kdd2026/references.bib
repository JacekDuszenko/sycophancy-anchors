% Sycophantic Anchors - Bibliography

% Thought Anchors paper (foundation methodology)
@misc{bogdan2025thought,
  title={Thought Anchors: Which LLM Reasoning Steps Matter?},
  author={Paul C. Bogdan and Uzay Macar and Neel Nanda and Arthur Conmy},
  year={2025},
  eprint={2506.19143},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.19143}
}

% Sycophancy papers
@misc{perez2022discovering,
      title={Discovering Language Model Behaviors with Model-Written Evaluations},
      author={Ethan Perez and others},
      year={2022},
      eprint={2212.09251},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09251},
}

@misc{sharma2023understanding,
      title={Towards Understanding Sycophancy in Language Models},
      author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Newton Cheng and Esin Durmus and Zac Hatfield-Dodds and Scott R. Johnston and Shauna Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
      year={2023},
      eprint={2310.13548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.13548},
}

% Chain-of-Thought papers
@misc{wei2022chain,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2022},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903},
}

@misc{lanham2023measuring,
      title={Measuring Faithfulness in Chain-of-Thought Reasoning},
      author={Tamera Lanham and Anna Chen and Ansh Radhakrishnan and Benoit Steiner and Carson Denison and Danny Hernandez and Dustin Li and Esin Durmus and Evan Hubinger and Jackson Kernion and Kamil{\.e} Luko{\v s}i{\=u}t{\.e} and Karina Nguyen and Newton Cheng and Nicholas Joseph and Nicholas Schiefer and Oliver Rausch and Robin Larson and Sam McCandlish and Sandipan Kundu and Saurav Kadavath and Shannon Yang and Thomas Henighan and Timothy Maxwell and Timothy Telleen-Lawton and Tristan Hume and Zac Hatfield-Dodds and Jared Kaplan and Jan Brauner and Samuel R. Bowman and Ethan Perez},
      year={2023},
      eprint={2307.13702},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.13702},
}

@misc{turpin2023language,
      title={Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
      author={Miles Turpin and Julian Michael and Ethan Perez and Samuel R. Bowman},
      year={2023},
      eprint={2305.04388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.04388},
}

% Latent knowledge and intervention
@misc{burns2022discovering,
      title={Discovering Latent Knowledge in Language Models Without Supervision},
      author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
      year={2022},
      eprint={2212.03827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03827},
}

@misc{li2024inference,
      title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
      author={Kenneth Li and Oam Patel and Fernanda Vi{\'e}gas and Hanspeter Pfister and Martin Wattenberg},
      year={2024},
      eprint={2306.03341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03341},
}

% ARC dataset
@article{clark2018think,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

% spaCy
@misc{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy}: Industrial-strength Natural Language Processing in Python},
  year = {2020},
  howpublished = {\url{https://github.com/explosion/spaCy}},
  note = {Version 3.x}
}

% DeepSeek R1
@article{deepseek2025r1,
   title={DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
   volume={645},
   ISSN={1476-4687},
   url={http://dx.doi.org/10.1038/s41586-025-09422-z},
   DOI={10.1038/s41586-025-09422-z},
   number={8081},
   journal={Nature},
   publisher={Springer Science and Business Media LLC},
   author={Guo, Daya and others},
   year={2025},
   month=sep, pages={633--638} }


% Claude Opus 4.5
@misc{anthropic2025claude,
  author = {Anthropic},
  title = {Claude Opus 4.5},
  year = {2025},
  url = {https://www.anthropic.com/claude},
}

% MONICA - concurrent work on sycophancy detection
@misc{hu2025monicarealtimemonitoringcalibration,
      title={MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models},
      author={Jingyu Hu and Shu Yang and Xilin Gong and Hongming Wang and Weiru Liu and Di Wang},
      year={2025},
      eprint={2511.06419},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2511.06419},
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models},
      author={Aaron Grattafiori and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783},
}


@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{falconllmteam2026falconh1rpushingreasoningfrontiers,
      title={Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling}, 
      author={Falcon LLM Team and Iheb Chaabane and Puneesh Khanna and Suhail Mohmad and Slim Frikha and Shi Hu and Abdalgader Abubaker and Reda Alami and Mikhail Lubinets and Mohamed El Amine Seddik and Hakim Hacid},
      year={2026},
      eprint={2601.02346},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2601.02346}, 
}

@misc{vaswani2017attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}